










Welcome to Ubuntu 24.04.2 LTS (GNU/Linux 5.15.167.4-microsoft-standard-WSL2 x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

 System information as of Thu Mar 20 14:49:24 PDT 2025

  System load:  0.24                Processes:             69
  Usage of /:   3.5% of 1006.85GB   Users logged in:       0
  Memory usage: 1%                  IPv4 address for eth0: 172.21.151.42
  Swap usage:   0%

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

This message is shown once a day. To disable it please create the
/home/m/.hushlogin file.
m@DESKTOP-MUB16F8:~$ source myenv/bin/activate
(myenv) m@DESKTOP-MUB16F8:~$ dir
m  myenv  qtip
(myenv) m@DESKTOP-MUB16F8:~$ cd qtip
(myenv) m@DESKTOP-MUB16F8:~/qtip$ dir
LICENSE                                                                                     lib
README.md                                                                                   model
assets                                                                                      qtip-kernels
eval                                                                                        quantize_llama
example.sh                                                                                  requirements.txt
fast-hadamard-transform                                                                     scripts
fast_hadamard_transform-1.0.4.post1+cu122torch2.2cxx11abiTRUE-cp311-cp311-linux_x86_64.whl
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-QTIP-3Bit --max_new_tokens 128 --streaming
I0320 14:53:11.037100 450 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 14:53:11.037272 450 utils.py:162] NumExpr defaulting to 16 threads.
I0320 14:53:11.396978 450 config.py:58] PyTorch version 2.4.0 available.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.99it/s]Some weights of LlamaForCausalLM were not initialized from the model checkpoint at relaxml/Llama-2-13b-QTIP-3Bit and are newly initialized: ['model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.32.mlp.down_proj.rcp', 'model.layers.32.mlp.down_proj.tp_rank', 'model.layers.32.mlp.gate_proj.rcp', 'model.layers.32.mlp.gate_proj.tp_rank', 'model.layers.32.mlp.up_proj.rcp', 'model.layers.32.mlp.up_proj.tp_rank', 'model.layers.32.self_attn.k_proj.rcp', 'model.layers.32.self_attn.k_proj.tp_rank', 'model.layers.32.self_attn.o_proj.rcp', 'model.layers.32.self_attn.o_proj.tp_rank', 'model.layers.32.self_attn.q_proj.rcp', 'model.layers.32.self_attn.q_proj.tp_rank', 'model.layers.32.self_attn.v_proj.rcp', 'model.layers.32.self_attn.v_proj.tp_rank', 'model.layers.33.mlp.down_proj.rcp', 'model.layers.33.mlp.down_proj.tp_rank', 'model.layers.33.mlp.gate_proj.rcp', 'model.layers.33.mlp.gate_proj.tp_rank', 'model.layers.33.mlp.up_proj.rcp', 'model.layers.33.mlp.up_proj.tp_rank', 'model.layers.33.self_attn.k_proj.rcp', 'model.layers.33.self_attn.k_proj.tp_rank', 'model.layers.33.self_attn.o_proj.rcp', 'model.layers.33.self_attn.o_proj.tp_rank', 'model.layers.33.self_attn.q_proj.rcp', 'model.layers.33.self_attn.q_proj.tp_rank', 'model.layers.33.self_attn.v_proj.rcp', 'model.layers.33.self_attn.v_proj.tp_rank', 'model.layers.34.mlp.down_proj.rcp', 'model.layers.34.mlp.down_proj.tp_rank', 'model.layers.34.mlp.gate_proj.rcp', 'model.layers.34.mlp.gate_proj.tp_rank', 'model.layers.34.mlp.up_proj.rcp', 'model.layers.34.mlp.up_proj.tp_rank', 'model.layers.34.self_attn.k_proj.rcp', 'model.layers.34.self_attn.k_proj.tp_rank', 'model.layers.34.self_attn.o_proj.rcp', 'model.layers.34.self_attn.o_proj.tp_rank', 'model.layers.34.self_attn.q_proj.rcp', 'model.layers.34.self_attn.q_proj.tp_rank', 'model.layers.34.self_attn.v_proj.rcp', 'model.layers.34.self_attn.v_proj.tp_rank', 'model.layers.35.mlp.down_proj.rcp', 'model.layers.35.mlp.down_proj.tp_rank', 'model.layers.35.mlp.gate_proj.rcp', 'model.layers.35.mlp.gate_proj.tp_rank', 'model.layers.35.mlp.up_proj.rcp', 'model.layers.35.mlp.up_proj.tp_rank', 'model.layers.35.self_attn.k_proj.rcp', 'model.layers.35.self_attn.k_proj.tp_rank', 'model.layers.35.self_attn.o_proj.rcp', 'model.layers.35.self_attn.o_proj.tp_rank', 'model.layers.35.self_attn.q_proj.rcp', 'model.layers.35.self_attn.q_proj.tp_rank', 'model.layers.35.self_attn.v_proj.rcp', 'model.layers.35.self_attn.v_proj.tp_rank', 'model.layers.36.mlp.down_proj.rcp', 'model.layers.36.mlp.down_proj.tp_rank', 'model.layers.36.mlp.gate_proj.rcp', 'model.layers.36.mlp.gate_proj.tp_rank', 'model.layers.36.mlp.up_proj.rcp', 'model.layers.36.mlp.up_proj.tp_rank', 'model.layers.36.self_attn.k_proj.rcp', 'model.layers.36.self_attn.k_proj.tp_rank', 'model.layers.36.self_attn.o_proj.rcp', 'model.layers.36.self_attn.o_proj.tp_rank', 'model.layers.36.self_attn.q_proj.rcp', 'model.layers.36.self_attn.q_proj.tp_rank', 'model.layers.36.self_attn.v_proj.rcp', 'model.layers.36.self_attn.v_proj.tp_rank', 'model.layers.37.mlp.down_proj.rcp', 'model.layers.37.mlp.down_proj.tp_rank', 'model.layers.37.mlp.gate_proj.rcp', 'model.layers.37.mlp.gate_proj.tp_rank', 'model.layers.37.mlp.up_proj.rcp', 'model.layers.37.mlp.up_proj.tp_rank', 'model.layers.37.self_attn.k_proj.rcp', 'model.layers.37.self_attn.k_proj.tp_rank', 'model.layers.37.self_attn.o_proj.rcp', 'model.layers.37.self_attn.o_proj.tp_rank', 'model.layers.37.self_attn.q_proj.rcp', 'model.layers.37.self_attn.q_proj.tp_rank', 'model.layers.37.self_attn.v_proj.rcp', 'model.layers.37.self_attn.v_proj.tp_rank', 'model.layers.38.mlp.down_proj.rcp', 'model.layers.38.mlp.down_proj.tp_rank', 'model.layers.38.mlp.gate_proj.rcp', 'model.layers.38.mlp.gate_proj.tp_rank', 'model.layers.38.mlp.up_proj.rcp', 'model.layers.38.mlp.up_proj.tp_rank', 'model.layers.38.self_attn.k_proj.rcp', 'model.layers.38.self_attn.k_proj.tp_rank', 'model.layers.38.self_attn.o_proj.rcp', 'model.layers.38.self_attn.o_proj.tp_rank', 'model.layers.38.self_attn.q_proj.rcp', 'model.layers.38.self_attn.q_proj.tp_rank', 'model.layers.38.self_attn.v_proj.rcp', 'model.layers.38.self_attn.v_proj.tp_rank', 'model.layers.39.mlp.down_proj.rcp', 'model.layers.39.mlp.down_proj.tp_rank', 'model.layers.39.mlp.gate_proj.rcp', 'model.layers.39.mlp.gate_proj.tp_rank', 'model.layers.39.mlp.up_proj.rcp', 'model.layers.39.mlp.up_proj.tp_rank', 'model.layers.39.self_attn.k_proj.rcp', 'model.layers.39.self_attn.k_proj.tp_rank', 'model.layers.39.self_attn.o_proj.rcp', 'model.layers.39.self_attn.o_proj.tp_rank', 'model.layers.39.self_attn.q_proj.rcp', 'model.layers.39.self_attn.q_proj.tp_rank', 'model.layers.39.self_attn.v_proj.rcp', 'model.layers.39.self_attn.v_proj.tp_rank', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tp_rank']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [00:21<00:00, 10.76s/it]Some weights of LlamaForCausalLM were not initialized from the model checkpoint at relaxml/Llama-2-13b-QTIP-3Bit and are newly initialized: ['model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.32.mlp.down_proj.rcp', 'model.layers.32.mlp.down_proj.tp_rank', 'model.layers.32.mlp.gate_proj.rcp', 'model.layers.32.mlp.gate_proj.tp_rank', 'model.layers.32.mlp.up_proj.rcp', 'model.layers.32.mlp.up_proj.tp_rank', 'model.layers.32.self_attn.k_proj.rcp', 'model.layers.32.self_attn.k_proj.tp_rank', 'model.layers.32.self_attn.o_proj.rcp', 'model.layers.32.self_attn.o_proj.tp_rank', 'model.layers.32.self_attn.q_proj.rcp', 'model.layers.32.self_attn.q_proj.tp_rank', 'model.layers.32.self_attn.v_proj.rcp', 'model.layers.32.self_attn.v_proj.tp_rank', 'model.layers.33.mlp.down_proj.rcp', 'model.layers.33.mlp.down_proj.tp_rank', 'model.layers.33.mlp.gate_proj.rcp', 'model.layers.33.mlp.gate_proj.tp_rank', 'model.layers.33.mlp.up_proj.rcp', 'model.layers.33.mlp.up_proj.tp_rank', 'model.layers.33.self_attn.k_proj.rcp', 'model.layers.33.self_attn.k_proj.tp_rank', 'model.layers.33.self_attn.o_proj.rcp', 'model.layers.33.self_attn.o_proj.tp_rank', 'model.layers.33.self_attn.q_proj.rcp', 'model.layers.33.self_attn.q_proj.tp_rank', 'model.layers.33.self_attn.v_proj.rcp', 'model.layers.33.self_attn.v_proj.tp_rank', 'model.layers.34.mlp.down_proj.rcp', 'model.layers.34.mlp.down_proj.tp_rank', 'model.layers.34.mlp.gate_proj.rcp', 'model.layers.34.mlp.gate_proj.tp_rank', 'model.layers.34.mlp.up_proj.rcp', 'model.layers.34.mlp.up_proj.tp_rank', 'model.layers.34.self_attn.k_proj.rcp', 'model.layers.34.self_attn.k_proj.tp_rank', 'model.layers.34.self_attn.o_proj.rcp', 'model.layers.34.self_attn.o_proj.tp_rank', 'model.layers.34.self_attn.q_proj.rcp', 'model.layers.34.self_attn.q_proj.tp_rank', 'model.layers.34.self_attn.v_proj.rcp', 'model.layers.34.self_attn.v_proj.tp_rank', 'model.layers.35.mlp.down_proj.rcp', 'model.layers.35.mlp.down_proj.tp_rank', 'model.layers.35.mlp.gate_proj.rcp', 'model.layers.35.mlp.gate_proj.tp_rank', 'model.layers.35.mlp.up_proj.rcp', 'model.layers.35.mlp.up_proj.tp_rank', 'model.layers.35.self_attn.k_proj.rcp', 'model.layers.35.self_attn.k_proj.tp_rank', 'model.layers.35.self_attn.o_proj.rcp', 'model.layers.35.self_attn.o_proj.tp_rank', 'model.layers.35.self_attn.q_proj.rcp', 'model.layers.35.self_attn.q_proj.tp_rank', 'model.layers.35.self_attn.v_proj.rcp', 'model.layers.35.self_attn.v_proj.tp_rank', 'model.layers.36.mlp.down_proj.rcp', 'model.layers.36.mlp.down_proj.tp_rank', 'model.layers.36.mlp.gate_proj.rcp', 'model.layers.36.mlp.gate_proj.tp_rank', 'model.layers.36.mlp.up_proj.rcp', 'model.layers.36.mlp.up_proj.tp_rank', 'model.layers.36.self_attn.k_proj.rcp', 'model.layers.36.self_attn.k_proj.tp_rank', 'model.layers.36.self_attn.o_proj.rcp', 'model.layers.36.self_attn.o_proj.tp_rank', 'model.layers.36.self_attn.q_proj.rcp', 'model.layers.36.self_attn.q_proj.tp_rank', 'model.layers.36.self_attn.v_proj.rcp', 'model.layers.36.self_attn.v_proj.tp_rank', 'model.layers.37.mlp.down_proj.rcp', 'model.layers.37.mlp.down_proj.tp_rank', 'model.layers.37.mlp.gate_proj.rcp', 'model.layers.37.mlp.gate_proj.tp_rank', 'model.layers.37.mlp.up_proj.rcp', 'model.layers.37.mlp.up_proj.tp_rank', 'model.layers.37.self_attn.k_proj.rcp', 'model.layers.37.self_attn.k_proj.tp_rank', 'model.layers.37.self_attn.o_proj.rcp', 'model.layers.37.self_attn.o_proj.tp_rank', 'model.layers.37.self_attn.q_proj.rcp', 'model.layers.37.self_attn.q_proj.tp_rank', 'model.layers.37.self_attn.v_proj.rcp', 'model.layers.37.self_attn.v_proj.tp_rank', 'model.layers.38.mlp.down_proj.rcp', 'model.layers.38.mlp.down_proj.tp_rank', 'model.layers.38.mlp.gate_proj.rcp', 'model.layers.38.mlp.gate_proj.tp_rank', 'model.layers.38.mlp.up_proj.rcp', 'model.layers.38.mlp.up_proj.tp_rank', 'model.layers.38.self_attn.k_proj.rcp', 'model.layers.38.self_attn.k_proj.tp_rank', 'model.layers.38.self_attn.o_proj.rcp', 'model.layers.38.self_attn.o_proj.tp_rank', 'model.layers.38.self_attn.q_proj.rcp', 'model.layers.38.self_attn.q_proj.tp_rank', 'model.layers.38.self_attn.v_proj.rcp', 'model.layers.38.self_attn.v_proj.tp_rank', 'model.layers.39.mlp.down_proj.rcp', 'model.layers.39.mlp.down_proj.tp_rank', 'model.layers.39.mlp.gate_proj.rcp', 'model.layers.39.mlp.gate_proj.tp_rank', 'model.layers.39.mlp.up_proj.rcp', 'model.layers.39.mlp.up_proj.tp_rank', 'model.layers.39.self_attn.k_proj.rcp', 'model.layers.39.self_attn.k_proj.tp_rank', 'model.layers.39.self_attn.o_proj.rcp', 'model.layers.39.self_attn.o_proj.tp_rank', 'model.layers.39.self_attn.q_proj.rcp', 'model.layers.39.self_attn.q_proj.tp_rank', 'model.layers.39.self_attn.v_proj.rcp', 'model.layers.39.self_attn.v_proj.tp_rank', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tp_rank']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
W0320 14:53:47.846000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.847000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.847000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.847000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q3 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.847000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.847000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.847000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.873000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.873000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.873000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.873000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.873000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.889000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.889000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.890000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.890000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:53:47.890000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:53:48.189000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:53:48.189000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:53:48.189000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:53:48.190000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:53:48.190000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.071000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.071000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.072000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.072000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.072000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.072000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z3 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.072000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.095000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.095000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.095000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.095000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.095000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.368000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.368000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.368000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.368000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0320 14:53:49.368000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.864000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.864000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.865000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x3 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.865000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.865000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x6 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.865000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.865000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.883000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.883000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.883000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.883000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0320 14:53:51.883000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0320 14:53:52.754000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0320 14:53:52.755000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0320 14:53:52.755000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0320 14:53:52.755000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0320 14:53:52.755000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.755000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.756000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q3 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.756000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.756000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.756000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.756000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.756000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.783000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.783000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.783000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.784000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.784000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.799000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.799000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.799000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.799000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.799000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.943000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.944000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.944000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.944000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:00.944000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.159000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.159000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.160000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.160000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.160000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.160000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.160000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z3 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.179000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.179000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.179000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.180000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.180000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.242000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.242000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.243000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.243000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:01.243000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.082000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.379000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.379000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.380000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.380000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x3 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.380000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.380000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.380000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.400000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.400000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.400000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.400000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.400000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.645000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.645000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.645000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.645000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.645000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:02.892000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.099000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q3 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.099000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.100000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.100000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.100000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.100000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.101000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.141000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.141000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.142000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.142000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.142000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.158000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.158000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.158000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.158000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.158000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.318000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.318000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.318000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.319000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.319000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.633000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.633000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.633000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.634000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.634000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.634000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.634000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z3 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.664000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.664000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.664000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.665000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.665000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.732000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.732000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.732000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.732000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:12.733000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:13.849000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:13.860000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:13.868000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:13.868000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.277000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.277000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.277000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.277000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.277000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.278000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.278000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.306000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.306000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.306000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.307000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.307000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.630000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.631000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.631000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.631000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.631000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.631000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.632000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.632000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.913000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.913000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.913000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.913000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 14:54:14.914000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:15.230000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0320 14:54:15.235000 139632014983296 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps5 is not in var_ranges, defaulting to unknown range.
W0320 14:54:21.216510 450 logging.py:328] Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
W0320 14:54:24.617490 450 warnings.py:110] /usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 140, in main
    ids, text, _ = generate(model, tokenizer, text,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/eval/interactive_gen.py", line 76, in generate
    next_token, logits = decode_one_tokens(model, next_token.clone(),
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/eval/interactive_gen.py", line 44, in decode_one_tokens
    logits = model(cur_token,
             ^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1437, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1242, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 965, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 831, in forward
    query_states = self.q_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/linear/quantized_linear.py", line 86, in forward
    return self.no_ckpt_forward(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/linear/quantized_linear.py", line 138, in no_ckpt_forward
    result = self.codebook_class(input,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/codebook/bitshift.py", line 444, in forward
    wrapper = getattr(
              ^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/_ops.py", line 1170, in __getattr__
    raise AttributeError(
AttributeError: '_OpNamespace' 'quip_lib' object has no attribute 'decompress_matvec_qtip_5120_1_5120_3'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-7b-QTIP-2Bit --max_new_tokens 256 --streaming
I0320 14:56:37.234805 963 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 14:56:37.235001 963 utils.py:162] NumExpr defaulting to 16 threads.
I0320 14:56:37.425209 963 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1354, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1135, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 914, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 454, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ nvidia-smi
Thu Mar 20 15:01:39 2025
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.77.01              Driver Version: 566.36         CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4060 Ti     On  |   00000000:02:00.0  On |                  N/A |
|  0%   33C    P8              6W /  165W |     412MiB /  16380MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A        33      G   /Xwayland                                   N/A      |
+-----------------------------------------------------------------------------------------+
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3
Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> exit()
(myenv) m@DESKTOP-MUB16F8:~/qtip$ bash example.sh
I0320 15:09:34.479543 1294 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 15:09:34.479750 1294 utils.py:162] NumExpr defaulting to 16 threads.
I0320 15:09:34.816052 1294 config.py:58] PyTorch version 2.4.0 available.
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 855/855 [00:00<00:00, 3.89MB/s]
model.safetensors.index.json: 100%|█████████████████████████████████████████████████████████████████████████████████████| 23.9k/23.9k [00:00<00:00, 7.45MB/s]
Downloading shards:   0%|                                                                                                              | 0/4 [00:00<?, ?it/s^model-00001-of-00004.safetensors:   1%|▋                                                                                | 41.9M/4.98G [00:12<23:34, 3.49MB/s]
Downloading shards:   0%|                                                                                                              | 0/4 [00:12<?, ?it/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/quantize_llama/quantize_finetune_llama.py", line 214, in <module>
    main(args)
  File "/home/m/qtip/quantize_llama/quantize_finetune_llama.py", line 110, in main
    model = AutoModelForCausalLM.from_pretrained(args.base_model,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3769, in from_pretrained
    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/utils/hub.py", line 1098, in get_checkpoint_shard_files
    cached_filename = cached_file(
                      ^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1389, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1915, in _download_to_tmp_and_move
    http_get(
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 549, in http_get
    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):
  File "/home/m/myenv/lib/python3.12/site-packages/requests/models.py", line 820, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 1066, in stream
    data = self.read(amt=amt, decode_content=decode_content)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 955, in read
    data = self._raw_read(amt)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 879, in _raw_read
    data = self._fp_read(amt, read1=read1) if not fp_closed else b""
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 862, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
           ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 479, in read
    s = self.fp.read(amt)
        ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/ssl.py", line 1252, in recv_into
    return self.read(nbytes, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/ssl.py", line 1104, in read
    return self._sslobj.read(len, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
^C
(myenv) m@DESKTOP-MUB16F8:~/qtip$ ^C
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit --max_new_tokens 25 --streaming
I0320 15:20:05.156525 1914 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 15:20:05.156694 1914 utils.py:162] NumExpr defaulting to 16 threads.
I0320 15:20:05.344646 1914 config.py:58] PyTorch version 2.4.0 available.
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1.12k/1.12k [00:00<00:00, 3.22MB/s]
model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3.85G/3.85G [18:28<00:00, 3.48MB/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1354, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1135, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 914, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 454, in __init__
    if config.quip_params['skip_list'] is None:
       ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'skip_list'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3
Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> import transformers
rt torch
>>> import torch
>>> model_id = "relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit"
ipeline >>> pipeline = transformers.pipeline(
...     "text-generation",
...     model=model_id,
...     device_map="auto",
... )




^C^CTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/pipelines/__init__.py", line 896, in pipeline
    framework, model = infer_framework_load_model(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/pipelines/base.py", line 288, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4014, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4324, in _load_pretrained_model
    model.apply(model._initialize_weights)
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  [Previous line repeated 2 more times]
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 1864, in _initialize_weights
    self._init_weights(module)
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 795, in _init_weights
    module.weight.data.normal_(mean=0.0, std=std)
KeyboardInterrupt
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> import transformers
id = ">>> import torch
relaxml/Llama-3>>>
>>> model_id = "relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit"
>>> pipeline = transformers.pipeline(
...     "text-generation",
...     model=model_id,
...     device_map="auto",
... )
"""
Generate a Python function that calculates the area of a rectangle given its length and width.
"""

# Instead of messages, provide the prompt directly
output = pipeline(
    prompt,
    max_new_tokens=22,
)

# Access the generated text
generated_text = output[0]['generated_text']
print(generated_text)

^CTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/pipelines/__init__.py", line 896, in pipeline
    framework, model = infer_framework_load_model(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/pipelines/base.py", line 288, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4014, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4324, in _load_pretrained_model
    model.apply(model._initialize_weights)
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  [Previous line repeated 2 more times]
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 1864, in _initialize_weights
    self._init_weights(module)
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 795, in _init_weights
    module.weight.data.normal_(mean=0.0, std=std)
KeyboardInterrupt
>>>
>>>
>>>
>>>
>>>
>>> import transformers
  # Im>>> import torch  # Import torch correctly
>>>
= "rela>>> model_id = "relaxml/Llama-3.1-8B-Instruct-QTIP-2Bit"  # Use correct capitalization
ipeline >>>
= transf>>> pipeline = transformers.pipeline(
...     "text-generation",
...     model=model_id,
    devi...     device_map="auto",
... )
ompt = """
Generate a Python function that calculates the area of a rectangle given its length and width.
"""  # Define the prompt

# Instead of messages, provide the prompt directly
output = pipeline(
    prompt,
    max_new_tokens=22,
)

# Access the generated text
generated_text = output[0]['generated_text']
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1.12k/1.12k [00:00<00:00, 7.16MB/s]
model.safetensors:   0%|▎                                                                                               | 10.5M/3.85G [00:02<14:10, 4.52MB/s]model.safetensors:   0%|▎                                                                                               | 10.5M/3.85G [00:04<25:51, 2.48MB/s]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/pipelines/__init__.py", line 896, in pipeline
    framework, model = infer_framework_load_model(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/pipelines/base.py", line 288, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3604, in from_pretrained
    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1389, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1915, in _download_to_tmp_and_move
    http_get(
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 549, in http_get
    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):
  File "/home/m/myenv/lib/python3.12/site-packages/requests/models.py", line 820, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 1066, in stream
    data = self.read(amt=amt, decode_content=decode_content)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 955, in read
    data = self._raw_read(amt)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 879, in _raw_read
    data = self._fp_read(amt, read1=read1) if not fp_closed else b""
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 862, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
           ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 479, in read
    s = self.fp.read(amt)
        ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/ssl.py", line 1252, in recv_into
    return self.read(nbytes, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/ssl.py", line 1104, in read
    return self._sslobj.read(len, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
>>>
KeyboardInterrupt
>>> import transformers
  # Im>>> import torch  # Import torch correctly
del_id >>>
= "rela>>> model_id = "relaxml/Llama-3.1-8B-Instruct-QTIP-2Bit"  # Use correct capitalization
ipeline >>>
= transf>>> pipeline = transformers.pipeline(
...     "text-generation",
...     model=model_id,
...     device_map="auto",
ompt = "... )
""
Generate a Python function that calculates the area of a rectangle given its length and width.
"""  # Define the prompt

# Instead of messages, provide the prompt directly
output = pipeline(
    prompt,
    max_new_tokens=22,
)

# Access the generated text
generated_text = output[0]['generated_text']
model.safetensors:   0%|▎                                                                                                       | 10.5M/3.85G [00:00<?, ?B/s]
model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3.85G/3.85G [18:20<00:00, 3.49MB/s]
Some weights of the model checkpoint at relaxml/Llama-3.1-8B-Instruct-QTIP-2Bit were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.trellis']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at relaxml/Llama-3.1-8B-Instruct-QTIP-2Bit and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 217/217 [00:00<00:00, 661kB/s]
Some parameters are on the meta device because they were offloaded to the cpu.
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 50.5k/50.5k [00:00<00:00, 6.50MB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 9.09M/9.09M [00:02<00:00, 3.47MB/s]
>>>
>>> prompt = """
... Generate a Python function that calculates the area of a rectangle given its length and width.
... """  # Define the prompt
>>>
>>> # Instead of messages, provide the prompt directly
>>> output = pipeline(
...     prompt,
...     max_new_tokens=22,
... )
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
>>>
>>> # Access the generated text
>>> generated_text = output[0]['generated_text']
>>> print(generated_text)

Generate a Python function that calculates the area of a rectangle given its length and width.
/protofcnfcnfcnfcnfcnfcnfcnfcnfcnožfcnfcnfcnfcnožfcnfcnfcnfcnfcnfcn
>>> exit()
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-QTIP-3Bit --max_new_tokens 256 --streaming
I0320 16:09:57.961628 2218 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 16:09:57.961804 2218 utils.py:162] NumExpr defaulting to 16 threads.
I0320 16:09:58.287476 2218 config.py:58] PyTorch version 2.4.0 available.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.43it/s]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at relaxml/Llama-2-13b-QTIP-3Bit and are newly initialized: ['model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.32.mlp.down_proj.rcp', 'model.layers.32.mlp.down_proj.tp_rank', 'model.layers.32.mlp.gate_proj.rcp', 'model.layers.32.mlp.gate_proj.tp_rank', 'model.layers.32.mlp.up_proj.rcp', 'model.layers.32.mlp.up_proj.tp_rank', 'model.layers.32.self_attn.k_proj.rcp', 'model.layers.32.self_attn.k_proj.tp_rank', 'model.layers.32.self_attn.o_proj.rcp', 'model.layers.32.self_attn.o_proj.tp_rank', 'model.layers.32.self_attn.q_proj.rcp', 'model.layers.32.self_attn.q_proj.tp_rank', 'model.layers.32.self_attn.v_proj.rcp', 'model.layers.32.self_attn.v_proj.tp_rank', 'model.layers.33.mlp.down_proj.rcp', 'model.layers.33.mlp.down_proj.tp_rank', 'model.layers.33.mlp.gate_proj.rcp', 'model.layers.33.mlp.gate_proj.tp_rank', 'model.layers.33.mlp.up_proj.rcp', 'model.layers.33.mlp.up_proj.tp_rank', 'model.layers.33.self_attn.k_proj.rcp', 'model.layers.33.self_attn.k_proj.tp_rank', 'model.layers.33.self_attn.o_proj.rcp', 'model.layers.33.self_attn.o_proj.tp_rank', 'model.layers.33.self_attn.q_proj.rcp', 'model.layers.33.self_attn.q_proj.tp_rank', 'model.layers.33.self_attn.v_proj.rcp', 'model.layers.33.self_attn.v_proj.tp_rank', 'model.layers.34.mlp.down_proj.rcp', 'model.layers.34.mlp.down_proj.tp_rank', 'model.layers.34.mlp.gate_proj.rcp', 'model.layers.34.mlp.gate_proj.tp_rank', 'model.layers.34.mlp.up_proj.rcp', 'model.layers.34.mlp.up_proj.tp_rank', 'model.layers.34.self_attn.k_proj.rcp', 'model.layers.34.self_attn.k_proj.tp_rank', 'model.layers.34.self_attn.o_proj.rcp', 'model.layers.34.self_attn.o_proj.tp_rank', 'model.layers.34.self_attn.q_proj.rcp', 'model.layers.34.self_attn.q_proj.tp_rank', 'model.layers.34.self_attn.v_proj.rcp', 'model.layers.34.self_attn.v_proj.tp_rank', 'model.layers.35.mlp.down_proj.rcp', 'model.layers.35.mlp.down_proj.tp_rank', 'model.layers.35.mlp.gate_proj.rcp', 'model.layers.35.mlp.gate_proj.tp_rank', 'model.layers.35.mlp.up_proj.rcp', 'model.layers.35.mlp.up_proj.tp_rank', 'model.layers.35.self_attn.k_proj.rcp', 'model.layers.35.self_attn.k_proj.tp_rank', 'model.layers.35.self_attn.o_proj.rcp', 'model.layers.35.self_attn.o_proj.tp_rank', 'model.layers.35.self_attn.q_proj.rcp', 'model.layers.35.self_attn.q_proj.tp_rank', 'model.layers.35.self_attn.v_proj.rcp', 'model.layers.35.self_attn.v_proj.tp_rank', 'model.layers.36.mlp.down_proj.rcp', 'model.layers.36.mlp.down_proj.tp_rank', 'model.layers.36.mlp.gate_proj.rcp', 'model.layers.36.mlp.gate_proj.tp_rank', 'model.layers.36.mlp.up_proj.rcp', 'model.layers.36.mlp.up_proj.tp_rank', 'model.layers.36.self_attn.k_proj.rcp', 'model.layers.36.self_attn.k_proj.tp_rank', 'model.layers.36.self_attn.o_proj.rcp', 'model.layers.36.self_attn.o_proj.tp_rank', 'model.layers.36.self_attn.q_proj.rcp', 'model.layers.36.self_attn.q_proj.tp_rank', 'model.layers.36.self_attn.v_proj.rcp', 'model.layers.36.self_attn.v_proj.tp_rank', 'model.layers.37.mlp.down_proj.rcp', 'model.layers.37.mlp.down_proj.tp_rank', 'model.layers.37.mlp.gate_proj.rcp', 'model.layers.37.mlp.gate_proj.tp_rank', 'model.layers.37.mlp.up_proj.rcp', 'model.layers.37.mlp.up_proj.tp_rank', 'model.layers.37.self_attn.k_proj.rcp', 'model.layers.37.self_attn.k_proj.tp_rank', 'model.layers.37.self_attn.o_proj.rcp', 'model.layers.37.self_attn.o_proj.tp_rank', 'model.layers.37.self_attn.q_proj.rcp', 'model.layers.37.self_attn.q_proj.tp_rank', 'model.layers.37.self_attn.v_proj.rcp', 'model.layers.37.self_attn.v_proj.tp_rank', 'model.layers.38.mlp.down_proj.rcp', 'model.layers.38.mlp.down_proj.tp_rank', 'model.layers.38.mlp.gate_proj.rcp', 'model.layers.38.mlp.gate_proj.tp_rank', 'model.layers.38.mlp.up_proj.rcp', 'model.layers.38.mlp.up_proj.tp_rank', 'model.layers.38.self_attn.k_proj.rcp', 'model.layers.38.self_attn.k_proj.tp_rank', 'model.layers.38.self_attn.o_proj.rcp', 'model.layers.38.self_attn.o_proj.tp_rank', 'model.layers.38.self_attn.q_proj.rcp', 'model.layers.38.self_attn.q_proj.tp_rank', 'model.layers.38.self_attn.v_proj.rcp', 'model.layers.38.self_attn.v_proj.tp_rank', 'model.layers.39.mlp.down_proj.rcp', 'model.layers.39.mlp.down_proj.tp_rank', 'model.layers.39.mlp.gate_proj.rcp', 'model.layers.39.mlp.gate_proj.tp_rank', 'model.layers.39.mlp.up_proj.rcp', 'model.layers.39.mlp.up_proj.tp_rank', 'model.layers.39.self_attn.k_proj.rcp', 'model.layers.39.self_attn.k_proj.tp_rank', 'model.layers.39.self_attn.o_proj.rcp', 'model.layers.39.self_attn.o_proj.tp_rank', 'model.layers.39.self_attn.q_proj.rcp', 'model.layers.39.self_attn.q_proj.tp_rank', 'model.layers.39.self_attn.v_proj.rcp', 'model.layers.39.self_attn.v_proj.tp_rank', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tp_rank']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:21<00:00, 10.79s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at relaxml/Llama-2-13b-QTIP-3Bit and are newly initialized: ['model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.32.mlp.down_proj.rcp', 'model.layers.32.mlp.down_proj.tp_rank', 'model.layers.32.mlp.gate_proj.rcp', 'model.layers.32.mlp.gate_proj.tp_rank', 'model.layers.32.mlp.up_proj.rcp', 'model.layers.32.mlp.up_proj.tp_rank', 'model.layers.32.self_attn.k_proj.rcp', 'model.layers.32.self_attn.k_proj.tp_rank', 'model.layers.32.self_attn.o_proj.rcp', 'model.layers.32.self_attn.o_proj.tp_rank', 'model.layers.32.self_attn.q_proj.rcp', 'model.layers.32.self_attn.q_proj.tp_rank', 'model.layers.32.self_attn.v_proj.rcp', 'model.layers.32.self_attn.v_proj.tp_rank', 'model.layers.33.mlp.down_proj.rcp', 'model.layers.33.mlp.down_proj.tp_rank', 'model.layers.33.mlp.gate_proj.rcp', 'model.layers.33.mlp.gate_proj.tp_rank', 'model.layers.33.mlp.up_proj.rcp', 'model.layers.33.mlp.up_proj.tp_rank', 'model.layers.33.self_attn.k_proj.rcp', 'model.layers.33.self_attn.k_proj.tp_rank', 'model.layers.33.self_attn.o_proj.rcp', 'model.layers.33.self_attn.o_proj.tp_rank', 'model.layers.33.self_attn.q_proj.rcp', 'model.layers.33.self_attn.q_proj.tp_rank', 'model.layers.33.self_attn.v_proj.rcp', 'model.layers.33.self_attn.v_proj.tp_rank', 'model.layers.34.mlp.down_proj.rcp', 'model.layers.34.mlp.down_proj.tp_rank', 'model.layers.34.mlp.gate_proj.rcp', 'model.layers.34.mlp.gate_proj.tp_rank', 'model.layers.34.mlp.up_proj.rcp', 'model.layers.34.mlp.up_proj.tp_rank', 'model.layers.34.self_attn.k_proj.rcp', 'model.layers.34.self_attn.k_proj.tp_rank', 'model.layers.34.self_attn.o_proj.rcp', 'model.layers.34.self_attn.o_proj.tp_rank', 'model.layers.34.self_attn.q_proj.rcp', 'model.layers.34.self_attn.q_proj.tp_rank', 'model.layers.34.self_attn.v_proj.rcp', 'model.layers.34.self_attn.v_proj.tp_rank', 'model.layers.35.mlp.down_proj.rcp', 'model.layers.35.mlp.down_proj.tp_rank', 'model.layers.35.mlp.gate_proj.rcp', 'model.layers.35.mlp.gate_proj.tp_rank', 'model.layers.35.mlp.up_proj.rcp', 'model.layers.35.mlp.up_proj.tp_rank', 'model.layers.35.self_attn.k_proj.rcp', 'model.layers.35.self_attn.k_proj.tp_rank', 'model.layers.35.self_attn.o_proj.rcp', 'model.layers.35.self_attn.o_proj.tp_rank', 'model.layers.35.self_attn.q_proj.rcp', 'model.layers.35.self_attn.q_proj.tp_rank', 'model.layers.35.self_attn.v_proj.rcp', 'model.layers.35.self_attn.v_proj.tp_rank', 'model.layers.36.mlp.down_proj.rcp', 'model.layers.36.mlp.down_proj.tp_rank', 'model.layers.36.mlp.gate_proj.rcp', 'model.layers.36.mlp.gate_proj.tp_rank', 'model.layers.36.mlp.up_proj.rcp', 'model.layers.36.mlp.up_proj.tp_rank', 'model.layers.36.self_attn.k_proj.rcp', 'model.layers.36.self_attn.k_proj.tp_rank', 'model.layers.36.self_attn.o_proj.rcp', 'model.layers.36.self_attn.o_proj.tp_rank', 'model.layers.36.self_attn.q_proj.rcp', 'model.layers.36.self_attn.q_proj.tp_rank', 'model.layers.36.self_attn.v_proj.rcp', 'model.layers.36.self_attn.v_proj.tp_rank', 'model.layers.37.mlp.down_proj.rcp', 'model.layers.37.mlp.down_proj.tp_rank', 'model.layers.37.mlp.gate_proj.rcp', 'model.layers.37.mlp.gate_proj.tp_rank', 'model.layers.37.mlp.up_proj.rcp', 'model.layers.37.mlp.up_proj.tp_rank', 'model.layers.37.self_attn.k_proj.rcp', 'model.layers.37.self_attn.k_proj.tp_rank', 'model.layers.37.self_attn.o_proj.rcp', 'model.layers.37.self_attn.o_proj.tp_rank', 'model.layers.37.self_attn.q_proj.rcp', 'model.layers.37.self_attn.q_proj.tp_rank', 'model.layers.37.self_attn.v_proj.rcp', 'model.layers.37.self_attn.v_proj.tp_rank', 'model.layers.38.mlp.down_proj.rcp', 'model.layers.38.mlp.down_proj.tp_rank', 'model.layers.38.mlp.gate_proj.rcp', 'model.layers.38.mlp.gate_proj.tp_rank', 'model.layers.38.mlp.up_proj.rcp', 'model.layers.38.mlp.up_proj.tp_rank', 'model.layers.38.self_attn.k_proj.rcp', 'model.layers.38.self_attn.k_proj.tp_rank', 'model.layers.38.self_attn.o_proj.rcp', 'model.layers.38.self_attn.o_proj.tp_rank', 'model.layers.38.self_attn.q_proj.rcp', 'model.layers.38.self_attn.q_proj.tp_rank', 'model.layers.38.self_attn.v_proj.rcp', 'model.layers.38.self_attn.v_proj.tp_rank', 'model.layers.39.mlp.down_proj.rcp', 'model.layers.39.mlp.down_proj.tp_rank', 'model.layers.39.mlp.gate_proj.rcp', 'model.layers.39.mlp.gate_proj.tp_rank', 'model.layers.39.mlp.up_proj.rcp', 'model.layers.39.mlp.up_proj.tp_rank', 'model.layers.39.self_attn.k_proj.rcp', 'model.layers.39.self_attn.k_proj.tp_rank', 'model.layers.39.self_attn.o_proj.rcp', 'model.layers.39.self_attn.o_proj.tp_rank', 'model.layers.39.self_attn.q_proj.rcp', 'model.layers.39.self_attn.q_proj.tp_rank', 'model.layers.39.self_attn.v_proj.rcp', 'model.layers.39.self_attn.v_proj.tp_rank', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tp_rank']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
W0320 16:10:32.692000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.692000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.692000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q3 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.692000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.693000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.693000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.693000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.718000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.718000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.718000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.718000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.718000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.734000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.734000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.734000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.735000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:32.735000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.043000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.043000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.043000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.043000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.043000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.889000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.889000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.889000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z3 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.889000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.889000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.889000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.890000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.906000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.906000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.907000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.907000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:33.907000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:34.139000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:34.140000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:34.140000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:34.140000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:34.140000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.036000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.036000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.036000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.037000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.037000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x3 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.037000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.037000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.054000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.054000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.054000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.054000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.055000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.939000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.939000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.939000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.940000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:36.940000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.431000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.432000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.432000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.432000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.432000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q3 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.433000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.433000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.460000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.460000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.461000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.461000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.461000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.476000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.476000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.476000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.476000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.476000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.622000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.623000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.623000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.623000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.623000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.840000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.841000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.841000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.841000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.841000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.841000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z3 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.842000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.860000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.860000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.860000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.860000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.860000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.922000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.922000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.922000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.922000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:42.922000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:43.775000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.077000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.077000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.077000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x3 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.078000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.078000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.078000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.078000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.098000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.098000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.098000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.098000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.098000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.342000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.342000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.342000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.342000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.343000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:44.593000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:50.958000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:50.959000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:50.959000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:50.959000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:50.959000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q3 is not in var_ranges, defaulting to unknown range.
W0320 16:10:50.960000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:50.960000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.000000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.000000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.000000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.000000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.000000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.015000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.015000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.015000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.016000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.016000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.174000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.174000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.174000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.175000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.175000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.487000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.487000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.487000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.488000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.488000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.488000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z3 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.488000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.520000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.520000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.520000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.520000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.520000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.587000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.587000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.587000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.587000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:51.587000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:52.739000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:52.754000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:52.762000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:52.762000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.180000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.181000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.181000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.181000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.181000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.182000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.182000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.210000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.210000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.211000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.211000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.211000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.540000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.541000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.541000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.541000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.541000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.541000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.542000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.542000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.829000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.829000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.829000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.829000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:10:53.829000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:54.155000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0320 16:10:54.160000 140274628984960 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps5 is not in var_ranges, defaulting to unknown range.
W0320 16:10:57.219883 2218 logging.py:328] Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
W0320 16:10:57.765087 2218 warnings.py:110] /usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 140, in main
    ids, text, _ = generate(model, tokenizer, text,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/eval/interactive_gen.py", line 76, in generate
    next_token, logits = decode_one_tokens(model, next_token.clone(),
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/eval/interactive_gen.py", line 44, in decode_one_tokens
    logits = model(cur_token,
             ^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1437, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1242, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 965, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 831, in forward
    query_states = self.q_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/linear/quantized_linear.py", line 86, in forward
    return self.no_ckpt_forward(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/linear/quantized_linear.py", line 138, in no_ckpt_forward
    result = self.codebook_class(input,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/codebook/bitshift.py", line 444, in forward
    wrapper = getattr(
              ^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/_ops.py", line 1170, in __getattr__
    raise AttributeError(
AttributeError: '_OpNamespace' 'quip_lib' object has no attribute 'decompress_matvec_qtip_5120_1_5120_3'
(myenv) m@DESKTOP-MUB16F8:~/qtip$
(myenv) m@DESKTOP-MUB16F8:~/qtip$
(myenv) m@DESKTOP-MUB16F8:~/qtip$
(myenv) m@DESKTOP-MUB16F8:~/qtip$
(myenv) m@DESKTOP-MUB16F8:~/qtip$
(myenv) m@DESKTOP-MUB16F8:~/qtip$
(myenv) m@DESKTOP-MUB16F8:~/qtip$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Fri_Jan__6_16:45:21_PST_2023
Cuda compilation tools, release 12.0, V12.0.140
Build cuda_12.0.r12.0/compiler.32267302_0
(myenv) m@DESKTOP-MUB16F8:~/qtip$ sudo apt-get install libcublas-dev-12-0
[sudo] password for m:
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
E: Unable to locate package libcublas-dev-12-0
(myenv) m@DESKTOP-MUB16F8:~/qtip$ pip install bitsandbytes --upgrade --force-reinstall --no-cache-dir
DEPRECATION: Loading egg at /home/m/myenv/lib/python3.12/site-packages/qtip_kernels_cuda-0.0.0-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330
DEPRECATION: Loading egg at /home/m/myenv/lib/python3.12/site-packages/fast_hadamard_transform-1.0.4.post1-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330
Collecting bitsandbytes
  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)
Collecting torch<3,>=2.0 (from bitsandbytes)
  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)
Collecting numpy>=1.17 (from bitsandbytes)
  Downloading numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.0/62.0 kB 1.4 MB/s eta 0:00:00
Collecting filelock (from torch<3,>=2.0->bitsandbytes)
  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)
Collecting typing-extensions>=4.10.0 (from torch<3,>=2.0->bitsandbytes)
  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)
Collecting networkx (from torch<3,>=2.0->bitsandbytes)
  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)
Collecting jinja2 (from torch<3,>=2.0->bitsandbytes)
  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting fsspec (from torch<3,>=2.0->bitsandbytes)
  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cusparselt-cu12==0.6.2 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting nvidia-nccl-cu12==2.21.5 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvtx-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)
  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting triton==3.2.0 (from torch<3,>=2.0->bitsandbytes)
  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)
Collecting setuptools (from torch<3,>=2.0->bitsandbytes)
  Downloading setuptools-77.0.3-py3-none-any.whl.metadata (6.6 kB)
Collecting sympy==1.13.1 (from torch<3,>=2.0->bitsandbytes)
  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes)
  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch<3,>=2.0->bitsandbytes)
  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.1/76.1 MB 3.5 MB/s eta 0:00:00
Downloading numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.1/16.1 MB 3.5 MB/s eta 0:00:00
Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)
   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/766.6 MB 3.5 MB/s eta 0:03:18
ERROR: Operation cancelled by user
(myenv) m@DESKTOP-MUB16F8:~/qtip$ ^C
(myenv) m@DESKTOP-MUB16F8:~/qtip$ pip show torch
DEPRECATION: Loading egg at /home/m/myenv/lib/python3.12/site-packages/qtip_kernels_cuda-0.0.0-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330
DEPRECATION: Loading egg at /home/m/myenv/lib/python3.12/site-packages/fast_hadamard_transform-1.0.4.post1-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330
Name: torch
Version: 2.4.0
Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Home-page: https://pytorch.org/
Author: PyTorch Team
Author-email: packages@pytorch.org
License: BSD-3
Location: /home/m/myenv/lib/python3.12/site-packages
Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, setuptools, sympy, triton, typing-extensions
Required-by: accelerate, fast_hadamard_transform, lm_eval, peft
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-QTIP-3Bit --max_new_tokens 256 --streaming
I0320 16:24:37.011349 2512 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 16:24:37.011515 2512 utils.py:162] NumExpr defaulting to 16 threads.
I0320 16:24:37.366615 2512 config.py:58] PyTorch version 2.4.0 available.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.09it/s]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at relaxml/Llama-2-13b-QTIP-3Bit and are newly initialized: ['model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.32.mlp.down_proj.rcp', 'model.layers.32.mlp.down_proj.tp_rank', 'model.layers.32.mlp.gate_proj.rcp', 'model.layers.32.mlp.gate_proj.tp_rank', 'model.layers.32.mlp.up_proj.rcp', 'model.layers.32.mlp.up_proj.tp_rank', 'model.layers.32.self_attn.k_proj.rcp', 'model.layers.32.self_attn.k_proj.tp_rank', 'model.layers.32.self_attn.o_proj.rcp', 'model.layers.32.self_attn.o_proj.tp_rank', 'model.layers.32.self_attn.q_proj.rcp', 'model.layers.32.self_attn.q_proj.tp_rank', 'model.layers.32.self_attn.v_proj.rcp', 'model.layers.32.self_attn.v_proj.tp_rank', 'model.layers.33.mlp.down_proj.rcp', 'model.layers.33.mlp.down_proj.tp_rank', 'model.layers.33.mlp.gate_proj.rcp', 'model.layers.33.mlp.gate_proj.tp_rank', 'model.layers.33.mlp.up_proj.rcp', 'model.layers.33.mlp.up_proj.tp_rank', 'model.layers.33.self_attn.k_proj.rcp', 'model.layers.33.self_attn.k_proj.tp_rank', 'model.layers.33.self_attn.o_proj.rcp', 'model.layers.33.self_attn.o_proj.tp_rank', 'model.layers.33.self_attn.q_proj.rcp', 'model.layers.33.self_attn.q_proj.tp_rank', 'model.layers.33.self_attn.v_proj.rcp', 'model.layers.33.self_attn.v_proj.tp_rank', 'model.layers.34.mlp.down_proj.rcp', 'model.layers.34.mlp.down_proj.tp_rank', 'model.layers.34.mlp.gate_proj.rcp', 'model.layers.34.mlp.gate_proj.tp_rank', 'model.layers.34.mlp.up_proj.rcp', 'model.layers.34.mlp.up_proj.tp_rank', 'model.layers.34.self_attn.k_proj.rcp', 'model.layers.34.self_attn.k_proj.tp_rank', 'model.layers.34.self_attn.o_proj.rcp', 'model.layers.34.self_attn.o_proj.tp_rank', 'model.layers.34.self_attn.q_proj.rcp', 'model.layers.34.self_attn.q_proj.tp_rank', 'model.layers.34.self_attn.v_proj.rcp', 'model.layers.34.self_attn.v_proj.tp_rank', 'model.layers.35.mlp.down_proj.rcp', 'model.layers.35.mlp.down_proj.tp_rank', 'model.layers.35.mlp.gate_proj.rcp', 'model.layers.35.mlp.gate_proj.tp_rank', 'model.layers.35.mlp.up_proj.rcp', 'model.layers.35.mlp.up_proj.tp_rank', 'model.layers.35.self_attn.k_proj.rcp', 'model.layers.35.self_attn.k_proj.tp_rank', 'model.layers.35.self_attn.o_proj.rcp', 'model.layers.35.self_attn.o_proj.tp_rank', 'model.layers.35.self_attn.q_proj.rcp', 'model.layers.35.self_attn.q_proj.tp_rank', 'model.layers.35.self_attn.v_proj.rcp', 'model.layers.35.self_attn.v_proj.tp_rank', 'model.layers.36.mlp.down_proj.rcp', 'model.layers.36.mlp.down_proj.tp_rank', 'model.layers.36.mlp.gate_proj.rcp', 'model.layers.36.mlp.gate_proj.tp_rank', 'model.layers.36.mlp.up_proj.rcp', 'model.layers.36.mlp.up_proj.tp_rank', 'model.layers.36.self_attn.k_proj.rcp', 'model.layers.36.self_attn.k_proj.tp_rank', 'model.layers.36.self_attn.o_proj.rcp', 'model.layers.36.self_attn.o_proj.tp_rank', 'model.layers.36.self_attn.q_proj.rcp', 'model.layers.36.self_attn.q_proj.tp_rank', 'model.layers.36.self_attn.v_proj.rcp', 'model.layers.36.self_attn.v_proj.tp_rank', 'model.layers.37.mlp.down_proj.rcp', 'model.layers.37.mlp.down_proj.tp_rank', 'model.layers.37.mlp.gate_proj.rcp', 'model.layers.37.mlp.gate_proj.tp_rank', 'model.layers.37.mlp.up_proj.rcp', 'model.layers.37.mlp.up_proj.tp_rank', 'model.layers.37.self_attn.k_proj.rcp', 'model.layers.37.self_attn.k_proj.tp_rank', 'model.layers.37.self_attn.o_proj.rcp', 'model.layers.37.self_attn.o_proj.tp_rank', 'model.layers.37.self_attn.q_proj.rcp', 'model.layers.37.self_attn.q_proj.tp_rank', 'model.layers.37.self_attn.v_proj.rcp', 'model.layers.37.self_attn.v_proj.tp_rank', 'model.layers.38.mlp.down_proj.rcp', 'model.layers.38.mlp.down_proj.tp_rank', 'model.layers.38.mlp.gate_proj.rcp', 'model.layers.38.mlp.gate_proj.tp_rank', 'model.layers.38.mlp.up_proj.rcp', 'model.layers.38.mlp.up_proj.tp_rank', 'model.layers.38.self_attn.k_proj.rcp', 'model.layers.38.self_attn.k_proj.tp_rank', 'model.layers.38.self_attn.o_proj.rcp', 'model.layers.38.self_attn.o_proj.tp_rank', 'model.layers.38.self_attn.q_proj.rcp', 'model.layers.38.self_attn.q_proj.tp_rank', 'model.layers.38.self_attn.v_proj.rcp', 'model.layers.38.self_attn.v_proj.tp_rank', 'model.layers.39.mlp.down_proj.rcp', 'model.layers.39.mlp.down_proj.tp_rank', 'model.layers.39.mlp.gate_proj.rcp', 'model.layers.39.mlp.gate_proj.tp_rank', 'model.layers.39.mlp.up_proj.rcp', 'model.layers.39.mlp.up_proj.tp_rank', 'model.layers.39.self_attn.k_proj.rcp', 'model.layers.39.self_attn.k_proj.tp_rank', 'model.layers.39.self_attn.o_proj.rcp', 'model.layers.39.self_attn.o_proj.tp_rank', 'model.layers.39.self_attn.q_proj.rcp', 'model.layers.39.self_attn.q_proj.tp_rank', 'model.layers.39.self_attn.v_proj.rcp', 'model.layers.39.self_attn.v_proj.tp_rank', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tp_rank']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:21<00:00, 10.59s/it]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at relaxml/Llama-2-13b-QTIP-3Bit and are newly initialized: ['model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.32.mlp.down_proj.rcp', 'model.layers.32.mlp.down_proj.tp_rank', 'model.layers.32.mlp.gate_proj.rcp', 'model.layers.32.mlp.gate_proj.tp_rank', 'model.layers.32.mlp.up_proj.rcp', 'model.layers.32.mlp.up_proj.tp_rank', 'model.layers.32.self_attn.k_proj.rcp', 'model.layers.32.self_attn.k_proj.tp_rank', 'model.layers.32.self_attn.o_proj.rcp', 'model.layers.32.self_attn.o_proj.tp_rank', 'model.layers.32.self_attn.q_proj.rcp', 'model.layers.32.self_attn.q_proj.tp_rank', 'model.layers.32.self_attn.v_proj.rcp', 'model.layers.32.self_attn.v_proj.tp_rank', 'model.layers.33.mlp.down_proj.rcp', 'model.layers.33.mlp.down_proj.tp_rank', 'model.layers.33.mlp.gate_proj.rcp', 'model.layers.33.mlp.gate_proj.tp_rank', 'model.layers.33.mlp.up_proj.rcp', 'model.layers.33.mlp.up_proj.tp_rank', 'model.layers.33.self_attn.k_proj.rcp', 'model.layers.33.self_attn.k_proj.tp_rank', 'model.layers.33.self_attn.o_proj.rcp', 'model.layers.33.self_attn.o_proj.tp_rank', 'model.layers.33.self_attn.q_proj.rcp', 'model.layers.33.self_attn.q_proj.tp_rank', 'model.layers.33.self_attn.v_proj.rcp', 'model.layers.33.self_attn.v_proj.tp_rank', 'model.layers.34.mlp.down_proj.rcp', 'model.layers.34.mlp.down_proj.tp_rank', 'model.layers.34.mlp.gate_proj.rcp', 'model.layers.34.mlp.gate_proj.tp_rank', 'model.layers.34.mlp.up_proj.rcp', 'model.layers.34.mlp.up_proj.tp_rank', 'model.layers.34.self_attn.k_proj.rcp', 'model.layers.34.self_attn.k_proj.tp_rank', 'model.layers.34.self_attn.o_proj.rcp', 'model.layers.34.self_attn.o_proj.tp_rank', 'model.layers.34.self_attn.q_proj.rcp', 'model.layers.34.self_attn.q_proj.tp_rank', 'model.layers.34.self_attn.v_proj.rcp', 'model.layers.34.self_attn.v_proj.tp_rank', 'model.layers.35.mlp.down_proj.rcp', 'model.layers.35.mlp.down_proj.tp_rank', 'model.layers.35.mlp.gate_proj.rcp', 'model.layers.35.mlp.gate_proj.tp_rank', 'model.layers.35.mlp.up_proj.rcp', 'model.layers.35.mlp.up_proj.tp_rank', 'model.layers.35.self_attn.k_proj.rcp', 'model.layers.35.self_attn.k_proj.tp_rank', 'model.layers.35.self_attn.o_proj.rcp', 'model.layers.35.self_attn.o_proj.tp_rank', 'model.layers.35.self_attn.q_proj.rcp', 'model.layers.35.self_attn.q_proj.tp_rank', 'model.layers.35.self_attn.v_proj.rcp', 'model.layers.35.self_attn.v_proj.tp_rank', 'model.layers.36.mlp.down_proj.rcp', 'model.layers.36.mlp.down_proj.tp_rank', 'model.layers.36.mlp.gate_proj.rcp', 'model.layers.36.mlp.gate_proj.tp_rank', 'model.layers.36.mlp.up_proj.rcp', 'model.layers.36.mlp.up_proj.tp_rank', 'model.layers.36.self_attn.k_proj.rcp', 'model.layers.36.self_attn.k_proj.tp_rank', 'model.layers.36.self_attn.o_proj.rcp', 'model.layers.36.self_attn.o_proj.tp_rank', 'model.layers.36.self_attn.q_proj.rcp', 'model.layers.36.self_attn.q_proj.tp_rank', 'model.layers.36.self_attn.v_proj.rcp', 'model.layers.36.self_attn.v_proj.tp_rank', 'model.layers.37.mlp.down_proj.rcp', 'model.layers.37.mlp.down_proj.tp_rank', 'model.layers.37.mlp.gate_proj.rcp', 'model.layers.37.mlp.gate_proj.tp_rank', 'model.layers.37.mlp.up_proj.rcp', 'model.layers.37.mlp.up_proj.tp_rank', 'model.layers.37.self_attn.k_proj.rcp', 'model.layers.37.self_attn.k_proj.tp_rank', 'model.layers.37.self_attn.o_proj.rcp', 'model.layers.37.self_attn.o_proj.tp_rank', 'model.layers.37.self_attn.q_proj.rcp', 'model.layers.37.self_attn.q_proj.tp_rank', 'model.layers.37.self_attn.v_proj.rcp', 'model.layers.37.self_attn.v_proj.tp_rank', 'model.layers.38.mlp.down_proj.rcp', 'model.layers.38.mlp.down_proj.tp_rank', 'model.layers.38.mlp.gate_proj.rcp', 'model.layers.38.mlp.gate_proj.tp_rank', 'model.layers.38.mlp.up_proj.rcp', 'model.layers.38.mlp.up_proj.tp_rank', 'model.layers.38.self_attn.k_proj.rcp', 'model.layers.38.self_attn.k_proj.tp_rank', 'model.layers.38.self_attn.o_proj.rcp', 'model.layers.38.self_attn.o_proj.tp_rank', 'model.layers.38.self_attn.q_proj.rcp', 'model.layers.38.self_attn.q_proj.tp_rank', 'model.layers.38.self_attn.v_proj.rcp', 'model.layers.38.self_attn.v_proj.tp_rank', 'model.layers.39.mlp.down_proj.rcp', 'model.layers.39.mlp.down_proj.tp_rank', 'model.layers.39.mlp.gate_proj.rcp', 'model.layers.39.mlp.gate_proj.tp_rank', 'model.layers.39.mlp.up_proj.rcp', 'model.layers.39.mlp.up_proj.tp_rank', 'model.layers.39.self_attn.k_proj.rcp', 'model.layers.39.self_attn.k_proj.tp_rank', 'model.layers.39.self_attn.o_proj.rcp', 'model.layers.39.self_attn.o_proj.tp_rank', 'model.layers.39.self_attn.q_proj.rcp', 'model.layers.39.self_attn.q_proj.tp_rank', 'model.layers.39.self_attn.v_proj.rcp', 'model.layers.39.self_attn.v_proj.tp_rank', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tp_rank']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
W0320 16:25:14.350000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.350000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.350000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q3 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.350000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.351000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.351000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.351000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.378000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.378000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.378000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.378000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.379000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.396000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.396000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.396000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.396000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.397000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.722000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.723000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.723000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.723000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:14.723000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.607000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.607000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.607000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.607000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.607000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z3 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.607000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.608000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.625000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.625000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.625000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.625000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.626000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.869000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.869000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.869000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.870000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:15.870000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.529000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.529000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.529000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.529000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.530000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x3 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.530000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.530000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.553000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.553000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.553000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.554000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:18.554000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:19.481000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:19.482000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:19.482000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:19.482000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:19.482000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.253000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.253000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.253000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q3 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.253000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.254000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.254000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.254000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.283000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.283000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.284000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.284000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.284000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.300000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.300000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.300000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.300000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.300000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.452000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.452000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.452000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.452000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.452000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.678000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.678000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.678000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.678000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.678000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z3 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.679000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.679000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.700000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.700000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.701000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.701000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.701000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.765000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.765000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.765000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.766000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:25.766000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.646000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.394000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.394000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.394000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.394000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.395000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.395000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x3 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.395000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.416000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.416000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.416000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.417000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.417000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.667000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.668000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.668000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.668000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.668000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:26.926000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.595000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.595000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.596000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q3 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.596000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.596000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.596000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.596000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.640000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.640000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.640000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.641000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.641000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.656000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.656000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.657000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.657000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.657000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.820000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.820000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.820000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.821000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:33.821000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.141000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.141000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.141000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.141000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.141000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z3 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.142000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.142000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.176000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.177000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.177000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.177000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.177000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.245000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.245000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.246000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.246000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:34.246000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.420000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.426000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.432000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.433000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.869000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.869000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.869000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.869000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.869000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.870000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.870000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.901000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.901000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.902000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.902000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:35.902000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.243000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.244000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.244000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.244000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.244000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.244000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.245000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.245000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.538000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.538000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.538000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.538000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.538000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.872000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0320 16:25:36.878000 139662048084096 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps5 is not in var_ranges, defaulting to unknown range.
W0320 16:25:40.031022 2512 logging.py:328] Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
W0320 16:25:40.601553 2512 warnings.py:110] /usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 140, in main
    ids, text, _ = generate(model, tokenizer, text,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/eval/interactive_gen.py", line 76, in generate
    next_token, logits = decode_one_tokens(model, next_token.clone(),
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/eval/interactive_gen.py", line 44, in decode_one_tokens
    logits = model(cur_token,
             ^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1437, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1242, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 965, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 831, in forward
    query_states = self.q_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/linear/quantized_linear.py", line 86, in forward
    return self.no_ckpt_forward(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/linear/quantized_linear.py", line 138, in no_ckpt_forward
    result = self.codebook_class(input,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/codebook/bitshift.py", line 444, in forward
    wrapper = getattr(
              ^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/torch/_ops.py", line 1170, in __getattr__
    raise AttributeError(
AttributeError: '_OpNamespace' 'quip_lib' object has no attribute 'decompress_matvec_qtip_5120_1_5120_3'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ pip list
DEPRECATION: Loading egg at /home/m/myenv/lib/python3.12/site-packages/qtip_kernels_cuda-0.0.0-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330
DEPRECATION: Loading egg at /home/m/myenv/lib/python3.12/site-packages/fast_hadamard_transform-1.0.4.post1-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330
Package                  Version
------------------------ -----------
absl-py                  2.1.0
accelerate               0.34.2
aiohappyeyeballs         2.6.1
aiohttp                  3.11.14
aiosignal                1.3.2
attrs                    25.3.0
certifi                  2025.1.31
chardet                  5.2.0
charset-normalizer       3.4.1
click                    8.1.8
colorama                 0.4.6
cuda-python              12.6.0
DataProperty             1.1.0
datasets                 2.20.0
dill                     0.3.8
evaluate                 0.4.3
fast_hadamard_transform  1.0.4.post1
fast_hadamard_transform  1.0.4.post1
filelock                 3.18.0
frozenlist               1.5.0
fsspec                   2024.5.0
glog                     0.3.1
huggingface-hub          0.24.0
idna                     3.10
Jinja2                   3.1.6
joblib                   1.4.2
jsonlines                4.0.0
lm_eval                  0.4.4
lxml                     5.3.1
MarkupSafe               3.0.2
mbstrdecoder             1.1.4
more-itertools           10.6.0
mpmath                   1.3.0
multidict                6.2.0
multiprocess             0.70.16
networkx                 3.4.2
ninja                    1.11.1.3
nltk                     3.9.1
numexpr                  2.10.2
numpy                    2.1.2
nvidia-cublas-cu12       12.1.3.1
nvidia-cuda-cupti-cu12   12.1.105
nvidia-cuda-nvrtc-cu12   12.1.105
nvidia-cuda-runtime-cu12 12.1.105
nvidia-cudnn-cu12        9.1.0.70
nvidia-cufft-cu12        11.0.2.54
nvidia-curand-cu12       10.3.2.106
nvidia-cusolver-cu12     11.4.5.107
nvidia-cusparse-cu12     12.1.0.106
nvidia-nccl-cu12         2.20.5
nvidia-nvjitlink-cu12    12.8.93
nvidia-nvtx-cu12         12.1.105
packaging                24.2
pandas                   2.2.3
pathvalidate             3.2.3
peft                     0.13.2
pip                      24.0
portalocker              3.1.1
propcache                0.3.0
psutil                   7.0.0
pyarrow                  19.0.1
pyarrow-hotfix           0.6
pybind11                 2.13.6
pytablewriter            1.2.1
python-dateutil          2.9.0.post0
python-gflags            3.1.2
pytz                     2025.1
PyYAML                   6.0.2
qtip_kernels_cuda        0.0.0
qtip_kernels_cuda        0.0.0
regex                    2024.11.6
requests                 2.32.3
rouge_score              0.1.2
sacrebleu                2.5.1
safetensors              0.5.3
scikit-learn             1.6.1
scipy                    1.14.1
setuptools               69.5.1
six                      1.17.0
sqlitedict               2.1.0
sympy                    1.13.3
tabledata                1.3.4
tabulate                 0.9.0
tcolorpy                 0.1.7
threadpoolctl            3.6.0
tokenizers               0.20.3
torch                    2.4.0
tqdm                     4.66.4
tqdm-multiprocess        0.0.11
transformers             4.45.2
triton                   3.0.0
typepy                   1.3.4
typing_extensions        4.12.2
tzdata                   2025.1
urllib3                  2.3.0
wheel                    0.45.1
word2number              1.1
xxhash                   3.5.0
yarl                     1.18.3
zstandard                0.23.0
(myenv) m@DESKTOP-MUB16F8:~/qtip$ git clone https://github.com/chu-tianxiang/QuIP-for-all.git
Cloning into 'QuIP-for-all'...
remote: Enumerating objects: 250, done.
remote: Counting objects: 100% (69/69), done.
remote: Compressing objects: 100% (14/14), done.
remote: Total 250 (delta 61), reused 55 (delta 55), pack-reused 181 (from 1)
Receiving objects: 100% (250/250), 254.53 KiB | 287.00 KiB/s, done.
Resolving deltas: 100% (163/163), done.
(myenv) m@DESKTOP-MUB16F8:~/qtip$ cd QuIP-for-all
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ dir
LICENSE    codebook      data.py              example_generate.py   qlinear.py  quantizer.py  quip_cuda        requirements.txt
README.md  constants.py  example_finetune.py  hadamard.safetensors  quant.py    quip.py       register_lib.py  utils.py
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ python3 example_generate.py -h
Traceback (most recent call last):
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 6, in <module>
    from quantizer import load_quantized_model
  File "/home/m/qtip/QuIP-for-all/quantizer.py", line 38, in <module>
    import register_lib
  File "/home/m/qtip/QuIP-for-all/register_lib.py", line 6, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ cd quip_cuda
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all/quip_cuda$ python3 setup_cuda.py install
python3: can't open file '/home/m/qtip/QuIP-for-all/quip_cuda/setup_cuda.py': [Errno 2] No such file or directory
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all/quip_cuda$ python3 setup.py install
running install
/home/m/myenv/lib/python3.12/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` directly.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
        ********************************************************************************

!!
  self.initialize_options()
/home/m/myenv/lib/python3.12/site-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` and ``easy_install``.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://github.com/pypa/setuptools/issues/917 for details.
        ********************************************************************************

!!
  self.initialize_options()
running bdist_egg
running egg_info
creating quiptools_cuda.egg-info
writing quiptools_cuda.egg-info/PKG-INFO
writing dependency_links to quiptools_cuda.egg-info/dependency_links.txt
writing top-level names to quiptools_cuda.egg-info/top_level.txt
writing manifest file 'quiptools_cuda.egg-info/SOURCES.txt'
reading manifest file 'quiptools_cuda.egg-info/SOURCES.txt'
writing manifest file 'quiptools_cuda.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-x86_64/egg
running install_lib
running build_ext
/home/m/myenv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.0) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.
  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))
/home/m/myenv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.0
  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')
building 'quiptools_cuda' extension
creating /home/m/qtip/QuIP-for-all/quip_cuda/build
creating /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312
/home/m/myenv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Emitting ninja build file /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] c++ -MMD -MF /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312/quiptools_wrapper.o.d -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -fPIC -I/home/m/myenv/lib/python3.12/site-packages/torch/include -I/home/m/myenv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/home/m/myenv/lib/python3.12/site-packages/torch/include/TH -I/home/m/myenv/lib/python3.12/site-packages/torch/include/THC -I/home/m/myenv/include -I/usr/include/python3.12 -c -c /home/m/qtip/QuIP-for-all/quip_cuda/quiptools_wrapper.cpp -o /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312/quiptools_wrapper.o -g -lineinfo -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=quiptools_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[2/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312/origin_order.o.d -I/home/m/myenv/lib/python3.12/site-packages/torch/include -I/home/m/myenv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/home/m/myenv/lib/python3.12/site-packages/torch/include/TH -I/home/m/myenv/lib/python3.12/site-packages/torch/include/THC -I/home/m/myenv/include -I/usr/include/python3.12 -c -c /home/m/qtip/QuIP-for-all/quip_cuda/origin_order.cu -o /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312/origin_order.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -g -Xcompiler -rdynamic -lineinfo -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=quiptools_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 -std=c++17
/home/m/qtip/QuIP-for-all/quip_cuda/origin_order.cu(41): warning #177-D: variable "KTilesPerWarp" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

[3/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312/e8p_gemv.o.d -I/home/m/myenv/lib/python3.12/site-packages/torch/include -I/home/m/myenv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/home/m/myenv/lib/python3.12/site-packages/torch/include/TH -I/home/m/myenv/lib/python3.12/site-packages/torch/include/THC -I/home/m/myenv/include -I/usr/include/python3.12 -c -c /home/m/qtip/QuIP-for-all/quip_cuda/e8p_gemv.cu -o /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312/e8p_gemv.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -g -Xcompiler -rdynamic -lineinfo -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=quiptools_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 -std=c++17
/home/m/qtip/QuIP-for-all/quip_cuda/e8p_gemv.cu(85): warning #177-D: variable "shared_weights" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

creating build/lib.linux-x86_64-cpython-312
x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312/e8p_gemv.o /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312/origin_order.o /home/m/qtip/QuIP-for-all/quip_cuda/build/temp.linux-x86_64-cpython-312/quiptools_wrapper.o -L/home/m/myenv/lib/python3.12/site-packages/torch/lib -L/usr/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-312/quiptools_cuda.cpython-312-x86_64-linux-gnu.so
creating build/bdist.linux-x86_64
creating build/bdist.linux-x86_64/egg
copying build/lib.linux-x86_64-cpython-312/quiptools_cuda.cpython-312-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg
creating stub loader for quiptools_cuda.cpython-312-x86_64-linux-gnu.so
byte-compiling build/bdist.linux-x86_64/egg/quiptools_cuda.py to quiptools_cuda.cpython-312.pyc
creating build/bdist.linux-x86_64/egg/EGG-INFO
copying quiptools_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO
copying quiptools_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
copying quiptools_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
copying quiptools_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt
zip_safe flag not set; analyzing archive contents...
__pycache__.quiptools_cuda.cpython-312: module references __file__
creating dist
creating 'dist/quiptools_cuda-0.0.0-py3.12-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it
removing 'build/bdist.linux-x86_64/egg' (and everything under it)
Processing quiptools_cuda-0.0.0-py3.12-linux-x86_64.egg
creating /home/m/myenv/lib/python3.12/site-packages/quiptools_cuda-0.0.0-py3.12-linux-x86_64.egg
Extracting quiptools_cuda-0.0.0-py3.12-linux-x86_64.egg to /home/m/myenv/lib/python3.12/site-packages
Adding quiptools-cuda 0.0.0 to easy-install.pth file

Installed /home/m/myenv/lib/python3.12/site-packages/quiptools_cuda-0.0.0-py3.12-linux-x86_64.egg
Processing dependencies for quiptools-cuda==0.0.0
Finished processing dependencies for quiptools-cuda==0.0.0
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all/quip_cuda$ python3
Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> from transformers import AutoTokenizer
import load_quantized_model
>>> from quantizer import load_quantized_model
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ModuleNotFoundError: No module named 'quantizer'
>>> from transformers import AutoTokenizer
>>> from quantizer import load_quantized_model
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ModuleNotFoundError: No module named 'quantizer'
>>> exit()
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all/quip_cuda$ cd ..
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ python3
Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from transformers import AutoTokenizer
antizer import load_quantized_model
>>> from quantizer import load_quantized_model
>>> quant_dir = "relaxml/Llama-2-13b-QTIP-3Bit"
>>> quant_model = load_quantized_model(quant_dir).cuda()
README.md: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 380kB/s]
.gitattributes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.52k/1.52k [00:00<00:00, 4.96MB/s]
Fetching 4 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.60it/s]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/qtip/QuIP-for-all/quantizer.py", line 816, in load_quantized_model
    with open(os.path.join(model_weights_path, QUIP_CONFIG)) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/m/.cache/huggingface/hub/models--relaxml--Llama-2-13b-QTIP-3Bit/snapshots/534631b97ef1b8acc2baf2e06cb6ebb8575ecd07/quantization_config.json'
>>> tokenizer = AutoTokenizer.from_pretrained(quant_dir)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 926, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 2192, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for 'relaxml/Llama-2-13b-QTIP-3Bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'relaxml/Llama-2-13b-QTIP-3Bit' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.
>>> input_ids = tokenizer.encode("The capital of France is", return_tensors="pt").cuda()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'tokenizer' is not defined. Did you mean: 'AutoTokenizer'?
>>> rint(tokenizer.decode(quant_model.generate(input_ids, do_sample=True)[0]))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'rint' is not defined. Did you mean: 'print'?
>>> exit()
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ python3 example_generate.py --model_path relaxml/Llama-2-13b-QTIP-3Bit --streaming --compile
Fetching 4 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 22459.46it/s]
Traceback (most recent call last):
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 125, in <module>
    main(args.model_path, args.compile, args.streaming, args.num_samples, args.max_new_tokens, args.top_k)
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 64, in main
    model = load_quantized_model(model_path)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/QuIP-for-all/quantizer.py", line 816, in load_quantized_model
    with open(os.path.join(model_weights_path, QUIP_CONFIG)) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/m/.cache/huggingface/hub/models--relaxml--Llama-2-13b-QTIP-3Bit/snapshots/534631b97ef1b8acc2baf2e06cb6ebb8575ecd07/quantization_config.json'
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ python3 example_generate.py --model_path relaxml/Llama-2-7b-QTIP-2Bit --streaming --compile
README.md: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 488kB/s]
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:00<00:00, 779kB/s]
.gitattributes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.52k/1.52k [00:00<00:00, 5.55MB/s]
Fetching 4 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 30.34it/s]
Traceback (most recent call last):
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 125, in <module>
    main(args.model_path, args.compile, args.streaming, args.num_samples, args.max_new_tokens, args.top_k)
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 64, in main
    model = load_quantized_model(model_path)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/QuIP-for-all/quantizer.py", line 816, in load_quantized_model
    with open(os.path.join(model_weights_path, QUIP_CONFIG)) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/m/.cache/huggingface/hub/models--relaxml--Llama-2-7b-QTIP-2Bit/snapshots/392a351a81d19c2af0ab0da9203e641cd8e441f9/quantization_config.json'
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ python3 example_generate.py -h
usage: example_generate.py [-h] [--model_path MODEL_PATH] [--streaming] [--num_samples NUM_SAMPLES] [--max_new_tokens MAX_NEW_TOKENS] [--top_k TOP_K]
                           [--compile]

Your CLI description.

options:
  -h, --help            show this help message and exit
  --model_path MODEL_PATH
                        Path to checkpoint
  --streaming           Whether to launch in stream mode
  --num_samples NUM_SAMPLES
                        Number of samples.
  --max_new_tokens MAX_NEW_TOKENS
                        Maximum number of new tokens.
  --top_k TOP_K         Top-k for sampling.
  --compile             Whether to compile the model.
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ python3 example_generate.py --model_path relaxml/Llama-2-7b-E8P-2Bit --streaming --compile
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 183/183 [00:00<00:00, 600kB/s]
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 981/981 [00:00<00:00, 3.06MB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 414/414 [00:00<00:00, 1.44MB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 776/776 [00:00<00:00, 3.34MB/s]
.gitattributes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.52k/1.52k [00:00<00:00, 6.51MB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 3.21MB/s]
tokenizer.model: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 1.88MB/s]
Fetching 7 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  5.44it/s]
Traceback (most recent call last):████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 1.89MB/s]
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 125, in <module>
    main(args.model_path, args.compile, args.streaming, args.num_samples, args.max_new_tokens, args.top_k)
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 64, in main
    model = load_quantized_model(model_path)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/QuIP-for-all/quantizer.py", line 816, in load_quantized_model
    with open(os.path.join(model_weights_path, QUIP_CONFIG)) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/m/.cache/huggingface/hub/models--relaxml--Llama-2-7b-E8P-2Bit/snapshots/22c75336223c885adf2cc95db52958cd6266c5bf/quantization_config.json'
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ python3 example_generate.py --model_path relaxml/Llama-2-7b-E8PRVQ-4Bit --streaming --compile
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 987/987 [00:00<00:00, 3.61MB/s]
.gitattributes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.52k/1.52k [00:00<00:00, 4.95MB/s]
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 183/183 [00:00<00:00, 850kB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 776/776 [00:00<00:00, 3.32MB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 414/414 [00:00<00:00, 1.87MB/s]
tokenizer.model: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 2.23MB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 2.05MB/s]
Fetching 7 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  4.92it/s]
Traceback (most recent call last):
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 125, in <module>
    main(args.model_path, args.compile, args.streaming, args.num_samples, args.max_new_tokens, args.top_k)
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 64, in main
    model = load_quantized_model(model_path)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/QuIP-for-all/quantizer.py", line 816, in load_quantized_model
    with open(os.path.join(model_weights_path, QUIP_CONFIG)) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/m/.cache/huggingface/hub/models--relaxml--Llama-2-7b-E8PRVQ-4Bit/snapshots/0f5eb118fd29fea3676a058bbd119e3ccd5b034d/quantization_config.json'
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ python3 example_generate.py --model_path relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit --streaming --compile
README.md: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 466kB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 50.5k/50.5k [00:00<00:00, 1.18MB/s]
.gitattributes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.52k/1.52k [00:00<00:00, 5.46MB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 9.09M/9.09M [00:02<00:00, 3.64MB/s]
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 217/217 [00:00<00:00, 879kB/s]
Fetching 6 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.65it/s]
Traceback (most recent call last):
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 125, in <module>
    main(args.model_path, args.compile, args.streaming, args.num_samples, args.max_new_tokens, args.top_k)
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 64, in main
    model = load_quantized_model(model_path)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/QuIP-for-all/quantizer.py", line 816, in load_quantized_model
    with open(os.path.join(model_weights_path, QUIP_CONFIG)) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/m/.cache/huggingface/hub/models--relaxml--Llama-3.1-8b-Instruct-QTIP-2Bit/snapshots/b67a9863702e87c44eb197250ae90ba910e75a66/quantization_config.json'
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ python3 example_generate.py --model_path relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit --streaming --compile
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 217/217 [00:00<00:00, 836kB/s]
README.md: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 264kB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 50.5k/50.5k [00:00<00:00, 1.20MB/s]
.gitattributes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.52k/1.52k [00:00<00:00, 6.49MB/s]
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1.12k/1.12k [00:00<00:00, 3.10MB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 9.09M/9.09M [00:02<00:00, 3.59MB/s]
Fetching 6 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.87it/s]
Traceback (most recent call last):██████████████████████████████████████████████████████████████████████████████████████| 9.09M/9.09M [00:02<00:00, 3.60MB/s]
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 125, in <module>
    main(args.model_path, args.compile, args.streaming, args.num_samples, args.max_new_tokens, args.top_k)
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 64, in main
    model = load_quantized_model(model_path)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/QuIP-for-all/quantizer.py", line 816, in load_quantized_model
    with open(os.path.join(model_weights_path, QUIP_CONFIG)) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/m/.cache/huggingface/hub/models--relaxml--Llama-3.1-8b-Instruct-QTIP-2Bit/snapshots/b67a9863702e87c44eb197250ae90ba910e75a66/quantization_config.json'
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ python3 example_generate.py --model_path relaxml/Llama-2-7b-E8P-2Bit --streaming --compile
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 981/981 [00:00<00:00, 3.57MB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 776/776 [00:00<00:00, 3.01MB/s]
.gitattributes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.52k/1.52k [00:00<00:00, 10.0MB/s]
generation_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 183/183 [00:00<00:00, 1.34MB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 414/414 [00:00<00:00, 1.87MB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 3.19MB/s]
tokenizer.model: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 998kB/s]
Fetching 7 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  5.74it/s]
Traceback (most recent call last):████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 1.00MB/s]
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 125, in <module>
    main(args.model_path, args.compile, args.streaming, args.num_samples, args.max_new_tokens, args.top_k)
  File "/home/m/qtip/QuIP-for-all/example_generate.py", line 64, in main
    model = load_quantized_model(model_path)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/QuIP-for-all/quantizer.py", line 816, in load_quantized_model
    with open(os.path.join(model_weights_path, QUIP_CONFIG)) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/m/.cache/huggingface/hub/models--relaxml--Llama-2-7b-E8P-2Bit/snapshots/22c75336223c885adf2cc95db52958cd6266c5bf/quantization_config.json'
(myenv) m@DESKTOP-MUB16F8:~/qtip/QuIP-for-all$ cd ..
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-7b-E8P-2Bit --max_new_tokens 256 --streaming
I0320 17:19:05.240871 3447 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 17:19:05.241031 3447 utils.py:162] NumExpr defaulting to 16 threads.
I0320 17:19:05.586670 3447 config.py:58] PyTorch version 2.4.0 available.
model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2.15G/2.15G [10:15<00:00, 3.49MB/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1354, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1135, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 914, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 445, in __init__
    td_x = config.quip_params['td_x']
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
KeyError: 'td_x'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 example_generate.py --model_path relaxml/Llama-2-13b-E8P-2Bit --streaming --compile
python3: can't open file '/home/m/qtip/example_generate.py': [Errno 2] No such file or directory
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 256 --streaming
I0320 17:33:49.865208 3618 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 17:33:49.865408 3618 utils.py:162] NumExpr defaulting to 16 threads.
I0320 17:33:50.189321 3618 config.py:58] PyTorch version 2.4.0 available.
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 982/982 [00:00<00:00, 2.71MB/s]
model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3.83G/3.83G [18:19<00:00, 3.49MB/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3886, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1354, in __init__
    self.model = LlamaModel(config)
                 ^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 1135, in __init__
    LlamaDecoderLayer(config, layer_idx)
  File "/home/m/qtip/model/llama.py", line 914, in __init__
    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/model/llama.py", line 445, in __init__
    td_x = config.quip_params['td_x']
           ~~~~~~~~~~~~~~~~~~^^^^^^^^
KeyError: 'td_x'
(myenv) m@DESKTOP-MUB16F8:~/qtip$ cat relaxml/Llama-2-13b-E8P-2Bit/config.json | jq .
cat: relaxml/Llama-2-13b-E8P-2Bit/config.json: No such file or directory
Command 'jq' not found, but can be installed with:
sudo snap install jq  # version 1.5+dfsg-1, or
sudo apt  install jq  # version 1.7.1-2
See 'snap info jq' for additional versions.
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3
Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> print("quip_params:", config.quip_params)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'config' is not defined
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
del_name = "relaxml/Llama-2-13b-E8P-2Bit"
model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir="./relaxml/")
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir="./relaxml/")
>>>
>>> model_name = "relaxml/Llama-2-13b-E8P-2Bit"
>>> model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir="./relaxml/")
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 982/982 [00:00<00:00, 3.35MB/s]
model.safetensors:   0%|▎                                                                                               | 10.5M/3.83G [00:02<17:18, 3.68MB/s]model.safetensors:   0%|▎                                                                                               | 10.5M/3.83G [00:04<28:47, 2.21MB/s]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3604, in from_pretrained
    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1389, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1915, in _download_to_tmp_and_move
    http_get(
  File "/home/m/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 549, in http_get
    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):
  File "/home/m/myenv/lib/python3.12/site-packages/requests/models.py", line 820, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 1066, in stream
    data = self.read(amt=amt, decode_content=decode_content)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 955, in read
    data = self._raw_read(amt)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 879, in _raw_read
    data = self._fp_read(amt, read1=read1) if not fp_closed else b""
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/urllib3/response.py", line 862, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
           ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 479, in read
    s = self.fp.read(amt)
        ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/ssl.py", line 1252, in recv_into
    return self.read(nbytes, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/ssl.py", line 1104, in read
    return self._sslobj.read(len, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
>>>
KeyboardInterrupt
>>> exit()
(myenv) m@DESKTOP-MUB16F8:~/qtip$ ls -lh relaxml/Llama-2-13b-E8P-2Bit/
ls: cannot access 'relaxml/Llama-2-13b-E8P-2Bit/': No such file or directory
(myenv) m@DESKTOP-MUB16F8:~/qtip$ ls -lh relaxml/Llama-2-13b-E8P-2Bit/
ls: cannot access 'relaxml/Llama-2-13b-E8P-2Bit/': No such file or directory
(myenv) m@DESKTOP-MUB16F8:~/qtip$ ls -lh relaxml/Llama-2-13b-E8P-2Bit
ls: cannot access 'relaxml/Llama-2-13b-E8P-2Bit': No such file or directory
(myenv) m@DESKTOP-MUB16F8:~/qtip$ wget -P relaxml/Llama-2-13b-E8P-2Bit/ https://huggingface.co/relaxml/Llama-2-13b-E8P-2Bit/resolve/main/config.json
--2025-03-20 18:29:31--  https://huggingface.co/relaxml/Llama-2-13b-E8P-2Bit/resolve/main/config.json
Resolving huggingface.co (huggingface.co)... 18.161.111.80, 18.161.111.71, 18.161.111.103, ...
Connecting to huggingface.co (huggingface.co)|18.161.111.80|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 982 [text/plain]
Saving to: ‘relaxml/Llama-2-13b-E8P-2Bit/config.json’

config.json                             100%[============================================================================>]     982  --.-KB/s    in 0s

2025-03-20 18:29:32 (188 MB/s) - ‘relaxml/Llama-2-13b-E8P-2Bit/config.json’ saved [982/982]

(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 256 --streaming
I0320 18:29:47.182069 3946 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 18:29:47.182233 3946 utils.py:162] NumExpr defaulting to 16 threads.
I0320 18:29:47.547758 3946 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3558, in from_pretrained
    raise EnvironmentError(
OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory relaxml/Llama-2-13b-E8P-2Bit.
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 256 --streaming
I0320 18:30:42.410378 4112 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 18:30:42.410611 4112 utils.py:162] NumExpr defaulting to 16 threads.
I0320 18:30:42.778392 4112 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3558, in from_pretrained
    raise EnvironmentError(
OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory relaxml/Llama-2-13b-E8P-2Bit.
(myenv) m@DESKTOP-MUB16F8:~/qtip$ huggingface-cli ls relaxml/Llama-2-13b-E8P-2Bit
usage: huggingface-cli <command> [<args>]
huggingface-cli: error: argument {download,upload,repo-files,env,login,whoami,logout,repo,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache,tag}: invalid choice: 'ls' (choose from 'download', 'upload', 'repo-files', 'env', 'login', 'whoami', 'logout', 'repo', 'lfs-enable-largefiles', 'lfs-multipart-upload', 'scan-cache', 'delete-cache', 'tag')
(myenv) m@DESKTOP-MUB16F8:~/qtip$ python3 -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-E8P-2Bit --max_new_tokens 256 --streaming
I0320 18:31:47.221276 4279 utils.py:149] Note: NumExpr detected 28 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0320 18:31:47.221540 4279 utils.py:162] NumExpr defaulting to 16 threads.
I0320 18:31:47.654993 4279 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/m/qtip/eval/interactive_gen.py", line 228, in <module>
    main(args.hf_path, not args.no_compile, args.streaming,
  File "/home/m/qtip/eval/interactive_gen.py", line 99, in main
    model, model_str = model_from_hf_path(hf_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/qtip/lib/utils/unsafe_import.py", line 35, in model_from_hf_path
    model = model_cls.from_pretrained(path,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3558, in from_pretrained
    raise EnvironmentError(
OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory relaxml/Llama-2-13b-E8P-2Bit.
(myenv) m@DESKTOP-MUB16F8:~/qtip$
